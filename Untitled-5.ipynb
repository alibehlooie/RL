{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym.envs.mujoco import HopperEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from env.custom_hopper import *\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomizedHopperEnv(HopperEnv):\n",
    "    def __init__(self, randomize_params=True):\n",
    "        super().__init__()\n",
    "        self.randomize_params = randomize_params\n",
    "        self.original_masses = self.model.body_mass.copy()\n",
    "        self.original_frictions = self.model.geom_friction.copy()\n",
    "\n",
    "    def reset(self):\n",
    "        if self.randomize_params:\n",
    "            self._randomize_parameters()\n",
    "        return super().reset()\n",
    "\n",
    "    def _randomize_parameters(self):\n",
    "        # Randomize masses\n",
    "        mass_multipliers = np.random.uniform(0.8, 1.2, size=self.model.body_mass.shape)\n",
    "        self.model.body_mass[:] = self.original_masses * mass_multipliers\n",
    "\n",
    "        # Randomize friction\n",
    "        friction_multipliers = np.random.uniform(0.8, 1.2, size=self.model.geom_friction.shape)\n",
    "        self.model.geom_friction[:] = self.original_frictions * friction_multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoDR:\n",
    "    def __init__(self, env, performance_threshold, adaptation_rate=0.1):\n",
    "        self.init_env = env\n",
    "        self.env = env\n",
    "        self.performance_threshold = performance_threshold\n",
    "        self.adaptation_rate = adaptation_rate\n",
    "        self.randomization_ranges = {\n",
    "            'mass': (0.8, 1.2),\n",
    "            'friction': (0.8, 1.2)\n",
    "        }\n",
    "\n",
    "    def update_ranges(self, performance):\n",
    "        if performance > self.performance_threshold:\n",
    "            # Increase randomization range\n",
    "            self._adjust_ranges(self.adaptation_rate)\n",
    "            # only call the randomize_env function if the ranges have been updated\n",
    "            print(\"updated ranges: \", self.randomization_ranges)\n",
    "            \n",
    "            print(\"Mass were:    \", self.env.model.body_mass[2:])\n",
    "            self.randomize_env()\n",
    "\n",
    "            print(\"Mass are now: \", self.env.model.body_mass[2:])\n",
    "        # else:\n",
    "            # Decrease randomization range\n",
    "            # if we are lower than the threshold, keep the weights as they are -> set the range to (1, 1)\n",
    "            # self.randomization_ranges = {\n",
    "                # 'mass': (1, 1),\n",
    "                # 'friction': (1, 1)\n",
    "            # }\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    def _adjust_ranges(self, adjustment):\n",
    "        for key in self.randomization_ranges:\n",
    "            lower, upper = self.randomization_ranges[key]\n",
    "            new_lower = min(1, max(0, lower - adjustment))\n",
    "            new_upper = max(1, min(2, upper + adjustment))\n",
    "            self.randomization_ranges[key] = (new_lower, new_upper)\n",
    "        \n",
    "    \n",
    "    def randomize_env(self):\n",
    "        # self.env.model.body_mass[2] = self.env.model.body_mass[2] * np.random.uniform(*self.randomization_ranges['mass'])\n",
    "        self.env.model.body_mass[2] = self.init_env.model.body_mass[2] * np.random.uniform(*self.randomization_ranges['mass'])\n",
    "        self.env.model.body_mass[3] = self.init_env.model.body_mass[3] * np.random.uniform(*self.randomization_ranges['mass'])\n",
    "        self.env.model.body_mass[4] = self.init_env.model.body_mass[4] * np.random.uniform(*self.randomization_ranges['mass'])\n",
    "        \n",
    "\n",
    "    def get_randomized_env(self):\n",
    "        self.env.randomize_params = True\n",
    "        self.env._randomize_parameters = self._custom_randomize\n",
    "        return self.env\n",
    "\n",
    "    def _custom_randomize(self):\n",
    "        mass_range = self.randomization_ranges['mass']\n",
    "        friction_range = self.randomization_ranges['friction']\n",
    "\n",
    "        mass_multipliers = np.random.uniform(*mass_range, size=self.env.model.body_mass.shape)\n",
    "        self.env.model.body_mass[:] = self.env.original_masses * mass_multipliers\n",
    "\n",
    "        friction_multipliers = np.random.uniform(*friction_range, size=self.env.model.geom_friction.shape)\n",
    "        self.env.model.geom_friction[:] = self.env.original_frictions * friction_multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CustomHopper-source-v0')\n",
    "# Initialize AutoDR\n",
    "auto_dr = AutoDR(env, performance_threshold = 250)  # Adjust the threshold as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf], (11,), float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = SAC(\"MlpPolicy\", auto_dr.get_randomized_env(), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps: 100, Mean reward: 21.46\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.8     |\n",
      "|    ep_rew_mean     | 19.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 119      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.24    |\n",
      "|    critic_loss     | 2.33     |\n",
      "|    ent_coef        | 0.995    |\n",
      "|    ent_coef_loss   | -0.0258  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18       |\n",
      "---------------------------------\n",
      "Timesteps: 200, Mean reward: 39.96\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.6     |\n",
      "|    ep_rew_mean     | 19.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 64       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 218      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.25    |\n",
      "|    critic_loss     | 0.476    |\n",
      "|    ent_coef        | 0.966    |\n",
      "|    ent_coef_loss   | -0.174   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 117      |\n",
      "---------------------------------\n",
      "Timesteps: 300, Mean reward: 29.59\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.5     |\n",
      "|    ep_rew_mean     | 21.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 66       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 343      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.37    |\n",
      "|    critic_loss     | 0.463    |\n",
      "|    ent_coef        | 0.93     |\n",
      "|    ent_coef_loss   | -0.367   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 242      |\n",
      "---------------------------------\n",
      "Timesteps: 400, Mean reward: 139.79\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.4     |\n",
      "|    ep_rew_mean     | 18.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 412      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.95    |\n",
      "|    critic_loss     | 0.5      |\n",
      "|    ent_coef        | 0.911    |\n",
      "|    ent_coef_loss   | -0.467   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 311      |\n",
      "---------------------------------\n",
      "Timesteps: 500, Mean reward: 77.46\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.8     |\n",
      "|    ep_rew_mean     | 22.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 66       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 549      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.28    |\n",
      "|    critic_loss     | 0.908    |\n",
      "|    ent_coef        | 0.875    |\n",
      "|    ent_coef_loss   | -0.668   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 448      |\n",
      "---------------------------------\n",
      "Timesteps: 600, Mean reward: 97.55\n",
      "Timesteps: 700, Mean reward: 78.41\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26       |\n",
      "|    ep_rew_mean     | 23.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 58       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 737      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.86    |\n",
      "|    critic_loss     | 0.601    |\n",
      "|    ent_coef        | 0.827    |\n",
      "|    ent_coef_loss   | -0.938   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 636      |\n",
      "---------------------------------\n",
      "Timesteps: 800, Mean reward: 92.81\n",
      "Timesteps: 900, Mean reward: 68.91\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | 29.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 56       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 947      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.9    |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.778    |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 846      |\n",
      "---------------------------------\n",
      "Timesteps: 1000, Mean reward: 73.84\n",
      "Timesteps: 1100, Mean reward: 88.67\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.2     |\n",
      "|    ep_rew_mean     | 34.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 57       |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1191     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.1    |\n",
      "|    critic_loss     | 0.937    |\n",
      "|    ent_coef        | 0.725    |\n",
      "|    ent_coef_loss   | -1.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1090     |\n",
      "---------------------------------\n",
      "Timesteps: 1200, Mean reward: 43.10\n",
      "Timesteps: 1300, Mean reward: 42.33\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.3     |\n",
      "|    ep_rew_mean     | 36.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 55       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 1355     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.7    |\n",
      "|    critic_loss     | 2.92     |\n",
      "|    ent_coef        | 0.692    |\n",
      "|    ent_coef_loss   | -1.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1254     |\n",
      "---------------------------------\n",
      "Timesteps: 1400, Mean reward: 43.96\n",
      "Timesteps: 1500, Mean reward: 260.28\n",
      "updated ranges:  {'mass': (0.7000000000000001, 1.3), 'friction': (0.7000000000000001, 1.3)}\n",
      "Mass were:     [3.92699082 2.71433605 5.0893801 ]\n",
      "Mass are now:  [4.96200174 3.08357209 5.21787716]\n",
      "Timesteps: 1600, Mean reward: 125.32\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 32.1     |\n",
      "|    ep_rew_mean     | 40.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 57       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 1645     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.7    |\n",
      "|    critic_loss     | 3.48     |\n",
      "|    ent_coef        | 0.638    |\n",
      "|    ent_coef_loss   | -1.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1544     |\n",
      "---------------------------------\n",
      "Timesteps: 1700, Mean reward: 130.91\n",
      "Timesteps: 1800, Mean reward: 117.96\n",
      "Timesteps: 1900, Mean reward: 114.45\n",
      "Timesteps: 2000, Mean reward: 110.84\n",
      "Timesteps: 2100, Mean reward: 109.03\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 34.9     |\n",
      "|    ep_rew_mean     | 47.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 56       |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2165     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -25.6    |\n",
      "|    critic_loss     | 2.1      |\n",
      "|    ent_coef        | 0.553    |\n",
      "|    ent_coef_loss   | -2.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2064     |\n",
      "---------------------------------\n",
      "Timesteps: 2200, Mean reward: 105.13\n",
      "Timesteps: 2300, Mean reward: 105.49\n",
      "Timesteps: 2400, Mean reward: 107.02\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.2     |\n",
      "|    ep_rew_mean     | 51.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 56       |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2463     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29      |\n",
      "|    critic_loss     | 2.83     |\n",
      "|    ent_coef        | 0.51     |\n",
      "|    ent_coef_loss   | -2.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2362     |\n",
      "---------------------------------\n",
      "Timesteps: 2500, Mean reward: 110.38\n",
      "Timesteps: 2600, Mean reward: 110.44\n",
      "Timesteps: 2700, Mean reward: 117.44\n",
      "Timesteps: 2800, Mean reward: 126.16\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.2     |\n",
      "|    ep_rew_mean     | 56.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 66       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2861     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -32.1    |\n",
      "|    critic_loss     | 3.35     |\n",
      "|    ent_coef        | 0.457    |\n",
      "|    ent_coef_loss   | -3.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2760     |\n",
      "---------------------------------\n",
      "Timesteps: 2900, Mean reward: 123.41\n",
      "Timesteps: 3000, Mean reward: 115.21\n",
      "Timesteps: 3100, Mean reward: 128.12\n",
      "Timesteps: 3200, Mean reward: 236.56\n",
      "Timesteps: 3300, Mean reward: 237.67\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 40.2     |\n",
      "|    ep_rew_mean     | 61.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 3369     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -37.3    |\n",
      "|    critic_loss     | 2.29     |\n",
      "|    ent_coef        | 0.397    |\n",
      "|    ent_coef_loss   | -3.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3268     |\n",
      "---------------------------------\n",
      "Timesteps: 3400, Mean reward: 241.33\n",
      "Timesteps: 3500, Mean reward: 255.33\n",
      "updated ranges:  {'mass': (0.6000000000000001, 1.4000000000000001), 'friction': (0.6000000000000001, 1.4000000000000001)}\n",
      "Mass were:     [4.96200174 3.08357209 5.21787716]\n",
      "Mass are now:  [3.24049654 2.15366937 3.91092397]\n",
      "Timesteps: 3600, Mean reward: 228.24\n",
      "Timesteps: 3700, Mean reward: 251.85\n",
      "updated ranges:  {'mass': (0.5000000000000001, 1.5000000000000002), 'friction': (0.5000000000000001, 1.5000000000000002)}\n",
      "Mass were:     [3.24049654 2.15366937 3.91092397]\n",
      "Mass are now:  [3.59681917 1.40323644 5.35191289]\n",
      "Timesteps: 3800, Mean reward: 195.00\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 42.8     |\n",
      "|    ep_rew_mean     | 67.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 3867     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -41.1    |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    ent_coef        | 0.346    |\n",
      "|    ent_coef_loss   | -3.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3766     |\n",
      "---------------------------------\n",
      "Timesteps: 3900, Mean reward: 135.16\n",
      "Timesteps: 4000, Mean reward: 215.17\n",
      "Timesteps: 4100, Mean reward: 206.86\n",
      "Timesteps: 4200, Mean reward: 169.00\n",
      "Timesteps: 4300, Mean reward: 141.98\n",
      "Timesteps: 4400, Mean reward: 234.57\n",
      "Timesteps: 4500, Mean reward: 135.52\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 44.1     |\n",
      "|    ep_rew_mean     | 70       |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4564     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -48      |\n",
      "|    critic_loss     | 2.84     |\n",
      "|    ent_coef        | 0.286    |\n",
      "|    ent_coef_loss   | -4.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4463     |\n",
      "---------------------------------\n",
      "Timesteps: 4600, Mean reward: 349.60\n",
      "updated ranges:  {'mass': (0.40000000000000013, 1.6000000000000003), 'friction': (0.40000000000000013, 1.6000000000000003)}\n",
      "Mass were:     [3.59681917 1.40323644 5.35191289]\n",
      "Mass are now:  [4.90075629 1.73895306 4.68823985]\n",
      "Timesteps: 4700, Mean reward: 150.24\n",
      "Timesteps: 4800, Mean reward: 309.95\n",
      "updated ranges:  {'mass': (0.30000000000000016, 1.7000000000000004), 'friction': (0.30000000000000016, 1.7000000000000004)}\n",
      "Mass were:     [4.90075629 1.73895306 4.68823985]\n",
      "Mass are now:  [6.03588859 2.77329255 3.9138705 ]\n",
      "Timesteps: 4900, Mean reward: 328.30\n",
      "updated ranges:  {'mass': (0.20000000000000015, 1.8000000000000005), 'friction': (0.20000000000000015, 1.8000000000000005)}\n",
      "Mass were:     [6.03588859 2.77329255 3.9138705 ]\n",
      "Mass are now:  [8.6340557  3.89602716 6.64655725]\n",
      "Timesteps: 5000, Mean reward: 166.93\n",
      "Timesteps: 5100, Mean reward: 363.70\n",
      "updated ranges:  {'mass': (0.10000000000000014, 1.9000000000000006), 'friction': (0.10000000000000014, 1.9000000000000006)}\n",
      "Mass were:     [8.6340557  3.89602716 6.64655725]\n",
      "Mass are now:  [15.8842654   1.61348034 11.81081184]\n",
      "Timesteps: 5200, Mean reward: 344.43\n",
      "updated ranges:  {'mass': (1.3877787807814457e-16, 2), 'friction': (1.3877787807814457e-16, 2)}\n",
      "Mass were:     [15.8842654   1.61348034 11.81081184]\n",
      "Mass are now:  [18.64511436  1.17487586  4.07318167]\n",
      "Timesteps: 5300, Mean reward: 332.82\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [18.64511436  1.17487586  4.07318167]\n",
      "Mass are now:  [14.91912711  1.71320153  7.04532308]\n",
      "Timesteps: 5400, Mean reward: 343.11\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [14.91912711  1.71320153  7.04532308]\n",
      "Mass are now:  [24.42949253  0.67041486  1.77295632]\n",
      "Timesteps: 5500, Mean reward: 340.03\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [24.42949253  0.67041486  1.77295632]\n",
      "Mass are now:  [19.30715966  0.87771065  2.13912911]\n",
      "Timesteps: 5600, Mean reward: 337.19\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [19.30715966  0.87771065  2.13912911]\n",
      "Mass are now:  [33.09330603  0.98782182  1.03911272]\n",
      "Timesteps: 5700, Mean reward: 322.71\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [33.09330603  0.98782182  1.03911272]\n",
      "Mass are now:  [19.39276216  1.10425896  0.29646739]\n",
      "Timesteps: 5800, Mean reward: 338.35\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [19.39276216  1.10425896  0.29646739]\n",
      "Mass are now:  [37.86628704  0.96431868  0.05967342]\n",
      "Timesteps: 5900, Mean reward: 316.47\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [37.86628704  0.96431868  0.05967342]\n",
      "Mass are now:  [4.71890018e+01 5.66331425e-02 2.76098983e-03]\n",
      "Timesteps: 6000, Mean reward: 252.06\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [4.71890018e+01 5.66331425e-02 2.76098983e-03]\n",
      "Mass are now:  [1.40910509e+01 6.97611798e-03 2.71107081e-03]\n",
      "Timesteps: 6100, Mean reward: 289.18\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [1.40910509e+01 6.97611798e-03 2.71107081e-03]\n",
      "Mass are now:  [2.64099166e+01 3.54002391e-03 4.32077662e-03]\n",
      "Timesteps: 6200, Mean reward: 234.83\n",
      "Timesteps: 6300, Mean reward: 251.78\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [2.64099166e+01 3.54002391e-03 4.32077662e-03]\n",
      "Mass are now:  [2.43652574e+01 2.44841412e-03 5.29771008e-03]\n",
      "Timesteps: 6400, Mean reward: 237.65\n",
      "Timesteps: 6500, Mean reward: 239.99\n",
      "Timesteps: 6600, Mean reward: 207.27\n",
      "Timesteps: 6700, Mean reward: 251.46\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [2.43652574e+01 2.44841412e-03 5.29771008e-03]\n",
      "Mass are now:  [3.04223429e+01 2.41815052e-04 2.21893138e-03]\n",
      "Timesteps: 6800, Mean reward: 272.28\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [3.04223429e+01 2.41815052e-04 2.21893138e-03]\n",
      "Mass are now:  [3.83575388e+01 6.64123041e-05 7.70324924e-04]\n",
      "Timesteps: 6900, Mean reward: 299.91\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [3.83575388e+01 6.64123041e-05 7.70324924e-04]\n",
      "Mass are now:  [3.78419203e+01 7.03570916e-05 5.37524836e-04]\n",
      "Timesteps: 7000, Mean reward: 298.36\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [3.78419203e+01 7.03570916e-05 5.37524836e-04]\n",
      "Mass are now:  [1.82184494e+01 1.47632504e-06 9.21646500e-05]\n",
      "Timesteps: 7100, Mean reward: 314.04\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [1.82184494e+01 1.47632504e-06 9.21646500e-05]\n",
      "Mass are now:  [1.89871770e+01 9.83676677e-07 1.54137445e-04]\n",
      "Timesteps: 7200, Mean reward: 305.95\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [1.89871770e+01 9.83676677e-07 1.54137445e-04]\n",
      "Mass are now:  [3.51379901e+01 9.08051950e-07 1.81696987e-04]\n",
      "Timesteps: 7300, Mean reward: 299.58\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [3.51379901e+01 9.08051950e-07 1.81696987e-04]\n",
      "Mass are now:  [4.59845302e+01 1.04847755e-06 3.14060816e-05]\n",
      "Timesteps: 7400, Mean reward: 280.81\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [4.59845302e+01 1.04847755e-06 3.14060816e-05]\n",
      "Mass are now:  [2.95806458e+01 2.32521270e-07 6.31663726e-07]\n",
      "Timesteps: 7500, Mean reward: 163.35\n",
      "Timesteps: 7600, Mean reward: 308.41\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [2.95806458e+01 2.32521270e-07 6.31663726e-07]\n",
      "Mass are now:  [5.18254441e+01 1.71343023e-07 1.72264403e-07]\n",
      "Timesteps: 7700, Mean reward: 122.29\n",
      "Timesteps: 7800, Mean reward: 273.93\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [5.18254441e+01 1.71343023e-07 1.72264403e-07]\n",
      "Mass are now:  [1.12502356e+01 3.23512612e-07 1.67584937e-07]\n",
      "Timesteps: 7900, Mean reward: 136.40\n",
      "Timesteps: 8000, Mean reward: 115.20\n",
      "Timesteps: 8100, Mean reward: 134.64\n",
      "Timesteps: 8200, Mean reward: 11.36\n",
      "Timesteps: 8300, Mean reward: 10.14\n",
      "Timesteps: 8400, Mean reward: 11.09\n",
      "Timesteps: 8500, Mean reward: 191.30\n",
      "Timesteps: 8600, Mean reward: 55.98\n",
      "Timesteps: 8700, Mean reward: 11.18\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 43.4     |\n",
      "|    ep_rew_mean     | 69       |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 65       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 8724     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -79.3    |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.102    |\n",
      "|    ent_coef_loss   | -2.87    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8623     |\n",
      "---------------------------------\n",
      "Timesteps: 8800, Mean reward: 12.44\n",
      "Timesteps: 8900, Mean reward: 10.56\n",
      "Timesteps: 9000, Mean reward: 283.44\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [1.12502356e+01 3.23512612e-07 1.67584937e-07]\n",
      "Mass are now:  [1.9791160e+01 4.8766086e-07 1.7505346e-07]\n",
      "Timesteps: 9100, Mean reward: 313.29\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [1.9791160e+01 4.8766086e-07 1.7505346e-07]\n",
      "Mass are now:  [2.84721311e+01 8.99138283e-07 2.27165240e-07]\n",
      "Timesteps: 9200, Mean reward: 295.25\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [2.84721311e+01 8.99138283e-07 2.27165240e-07]\n",
      "Mass are now:  [4.07936394e+01 1.54661252e-06 3.99051640e-07]\n",
      "Timesteps: 9300, Mean reward: 261.85\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [4.07936394e+01 1.54661252e-06 3.99051640e-07]\n",
      "Mass are now:  [5.30039968e+01 2.99036072e-06 4.57411667e-07]\n",
      "Timesteps: 9400, Mean reward: 239.21\n",
      "Timesteps: 9500, Mean reward: 270.21\n",
      "updated ranges:  {'mass': (0, 2), 'friction': (0, 2)}\n",
      "Mass were:     [5.30039968e+01 2.99036072e-06 4.57411667e-07]\n",
      "Mass are now:  [9.56988895e+01 3.66094767e-06 3.69998998e-07]\n",
      "Timesteps: 9600, Mean reward: 214.05\n",
      "Timesteps: 9700, Mean reward: 205.66\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 45.1     |\n",
      "|    ep_rew_mean     | 73       |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 62       |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 9799     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -87.6    |\n",
      "|    critic_loss     | 6.19     |\n",
      "|    ent_coef        | 0.0822   |\n",
      "|    ent_coef_loss   | -2.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9698     |\n",
      "---------------------------------\n",
      "Timesteps: 9800, Mean reward: 211.85\n",
      "Timesteps: 9900, Mean reward: 208.46\n",
      "Timesteps: 10000, Mean reward: 208.56\n"
     ]
    }
   ],
   "source": [
    "total_timesteps = 100000\n",
    "eval_interval = 100\n",
    "timesteps = 0\n",
    "\n",
    "while timesteps < total_timesteps:\n",
    "    # Train for a bit\n",
    "    model.learn(total_timesteps=eval_interval, reset_num_timesteps=False)\n",
    "    timesteps += eval_interval\n",
    "\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mean_reward, _ = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "    print(f\"Timesteps: {timesteps}, Mean reward: {mean_reward:.2f}\")\n",
    "\n",
    "    # Update AutoDR ranges\n",
    "    auto_dr.update_ranges(mean_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
