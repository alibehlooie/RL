{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - REINFORCE (Vanilla Policy Gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "\n",
    "from env.custom_hopper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Box([-1. -1. -1.], [1. 1. 1.], (3,), float32)\n",
      "State space: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf], (11,), float64)\n",
      "Dynamics parameters: [2.53429174 3.92699082 2.71433605 5.0893801 ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CustomHopper-source-v0')\n",
    "\n",
    "print('Action space:', env.action_space)\n",
    "print('State space:', env.observation_space)\n",
    "print('Dynamics parameters:', env.get_parameters())\n",
    "\n",
    "observation_space_dim = env.observation_space.shape[-1]\n",
    "action_space_dim = env.action_space.shape[-1]\n",
    "\n",
    "seed_value = 1234\n",
    "n_episodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Return: 372.0458397563965\n",
      "Episode: 1 | Return: 368.2702959528808\n",
      "Episode: 2 | Return: 231.33360258129952\n",
      "Episode: 3 | Return: 373.0867081360459\n",
      "Episode: 4 | Return: 350.08055084521726\n",
      "Episode: 5 | Return: 582.7491480306913\n",
      "Episode: 6 | Return: 521.3258152743631\n",
      "Episode: 7 | Return: 341.4859424331279\n",
      "Episode: 8 | Return: 272.0349742523086\n",
      "Episode: 9 | Return: 326.8920841747951\n",
      "Episode: 10 | Return: 276.69841873845326\n",
      "Episode: 11 | Return: 328.74050984505345\n",
      "Episode: 12 | Return: 344.4689223000142\n",
      "Episode: 13 | Return: 363.00528601493227\n",
      "Episode: 14 | Return: 772.7539171098514\n",
      "Episode: 15 | Return: 364.26280387523013\n",
      "Episode: 16 | Return: 243.38486056399816\n",
      "Episode: 17 | Return: 336.50032377433325\n",
      "Episode: 18 | Return: 364.27791995880375\n",
      "Episode: 19 | Return: 331.2671943934878\n",
      "Episode: 20 | Return: 343.48674366356215\n",
      "Episode: 21 | Return: 362.5634711123964\n",
      "Episode: 22 | Return: 740.7889543833462\n",
      "Episode: 23 | Return: 444.9906907348974\n",
      "Episode: 24 | Return: 360.2046985595709\n",
      "Episode: 25 | Return: 489.69239124060346\n",
      "Episode: 26 | Return: 321.0643429191645\n",
      "Episode: 27 | Return: 207.6094992286136\n",
      "Episode: 28 | Return: 309.35152890355005\n",
      "Episode: 29 | Return: 352.04371063565884\n",
      "Episode: 30 | Return: 339.968870518588\n",
      "Episode: 31 | Return: 467.1924451152129\n",
      "Episode: 32 | Return: 334.663440750708\n",
      "Episode: 33 | Return: 355.2159180606452\n",
      "Episode: 34 | Return: 393.91789950212944\n",
      "Episode: 35 | Return: 402.08451632649536\n",
      "Episode: 36 | Return: 348.3931636729722\n",
      "Episode: 37 | Return: 353.69759424338844\n",
      "Episode: 38 | Return: 367.89904374080714\n",
      "Episode: 39 | Return: 258.9479396419347\n",
      "Episode: 40 | Return: 344.1123080465056\n",
      "Episode: 41 | Return: 362.5964149502222\n",
      "Episode: 42 | Return: 230.1934704110638\n",
      "Episode: 43 | Return: 490.26723647814873\n",
      "Episode: 44 | Return: 315.5575858664446\n",
      "Episode: 45 | Return: 229.5385253103565\n",
      "Episode: 46 | Return: 352.81719468764317\n",
      "Episode: 47 | Return: 334.151940086279\n",
      "Episode: 48 | Return: 356.4755536578312\n",
      "Episode: 49 | Return: 484.92093631324076\n"
     ]
    }
   ],
   "source": [
    "from agent_reinforce import Agent, Policy\n",
    "\n",
    "reward_list_reinforce = []\n",
    "\n",
    "model = \"REINFORCE-100k.mdl\"\n",
    "\n",
    "policy = Policy(observation_space_dim, action_space_dim)\n",
    "policy.load_state_dict(torch.load(model), strict=True)\n",
    "\n",
    "agent = Agent(policy, device = \"cpu\")\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    done = False\n",
    "    test_reward = 0\n",
    "    env.seed(seed_value + episode)\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "\n",
    "        action, _ = agent.get_action(state, evaluation=True)\n",
    "\n",
    "        state, reward, done, info = env.step(action.detach().cpu().numpy())\n",
    "\n",
    "        test_reward += reward\n",
    "        # env.render()\n",
    "    reward_list_reinforce.append(test_reward)\n",
    "    print(f\"Episode: {episode} | Return: {test_reward}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Return: 340.0227342709311\n",
      "Episode: 1 | Return: 167.98717826572607\n",
      "Episode: 2 | Return: 374.5498351279817\n",
      "Episode: 3 | Return: 374.7337886845524\n",
      "Episode: 4 | Return: 188.05971077338134\n",
      "Episode: 5 | Return: 388.15119892626666\n",
      "Episode: 6 | Return: 188.9818675237329\n",
      "Episode: 7 | Return: 197.83860713641332\n",
      "Episode: 8 | Return: 365.0956455743209\n",
      "Episode: 9 | Return: 368.5039534422479\n",
      "Episode: 10 | Return: 373.2816643524105\n",
      "Episode: 11 | Return: 316.174922260255\n",
      "Episode: 12 | Return: 316.54662962220664\n",
      "Episode: 13 | Return: 253.3717649266839\n",
      "Episode: 14 | Return: 404.31497464261304\n",
      "Episode: 15 | Return: 369.727774472464\n",
      "Episode: 16 | Return: 438.0005140418449\n",
      "Episode: 17 | Return: 389.7525418821754\n",
      "Episode: 18 | Return: 374.1237761266403\n",
      "Episode: 19 | Return: 375.9434284249575\n",
      "Episode: 20 | Return: 234.4117427541253\n",
      "Episode: 21 | Return: 419.5283164641726\n",
      "Episode: 22 | Return: 375.9712930437545\n",
      "Episode: 23 | Return: 372.5761466945517\n",
      "Episode: 24 | Return: 377.9571145648134\n",
      "Episode: 25 | Return: 214.78168030183687\n",
      "Episode: 26 | Return: 371.00877783334147\n",
      "Episode: 27 | Return: 377.0945817836979\n",
      "Episode: 28 | Return: 375.00990182694994\n",
      "Episode: 29 | Return: 357.01534902912977\n",
      "Episode: 30 | Return: 377.0731792784235\n",
      "Episode: 31 | Return: 423.5619658000869\n",
      "Episode: 32 | Return: 381.4531790557565\n",
      "Episode: 33 | Return: 359.93635672312786\n",
      "Episode: 34 | Return: 374.88424145604773\n",
      "Episode: 35 | Return: 372.90681755346293\n",
      "Episode: 36 | Return: 371.5325509347516\n",
      "Episode: 37 | Return: 374.4034347948305\n",
      "Episode: 38 | Return: 253.1334066447524\n",
      "Episode: 39 | Return: 343.62528946364307\n",
      "Episode: 40 | Return: 367.51508223190376\n",
      "Episode: 41 | Return: 379.69687911698406\n",
      "Episode: 42 | Return: 382.80031935231887\n",
      "Episode: 43 | Return: 222.11863514560332\n",
      "Episode: 44 | Return: 379.36538966181297\n",
      "Episode: 45 | Return: 162.90811004048524\n",
      "Episode: 46 | Return: 327.2257841242412\n",
      "Episode: 47 | Return: 363.4117161559839\n",
      "Episode: 48 | Return: 385.45375359781974\n",
      "Episode: 49 | Return: 375.8306428927289\n"
     ]
    }
   ],
   "source": [
    "from agent_reinforce import Agent, Policy\n",
    "\n",
    "reward_list_reinforce_with_baseline = []\n",
    "\n",
    "model = \"REINFORCE-with_baseline_100k.mdl\"\n",
    "\n",
    "policy = Policy(observation_space_dim, action_space_dim)\n",
    "policy.load_state_dict(torch.load(model), strict=True)\n",
    "\n",
    "agent = Agent(policy, device = \"cpu\")\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    done = False\n",
    "    test_reward = 0\n",
    "    env.seed(seed_value + episode)\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "\n",
    "        action, _ = agent.get_action(state, evaluation=True)\n",
    "\n",
    "        state, reward, done, info = env.step(action.detach().cpu().numpy())\n",
    "\n",
    "        test_reward += reward\n",
    "        # env.render()\n",
    "    reward_list_reinforce_with_baseline.append(test_reward)\n",
    "    print(f\"Episode: {episode} | Return: {test_reward}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Return: 300.2326721777124\n",
      "Episode: 1 | Return: 314.7175922495674\n",
      "Episode: 2 | Return: 262.41011069660203\n",
      "Episode: 3 | Return: 239.16858139373315\n",
      "Episode: 4 | Return: 302.0821774505435\n",
      "Episode: 5 | Return: 348.36643468468924\n",
      "Episode: 6 | Return: 224.42370003888544\n",
      "Episode: 7 | Return: 265.8663167592419\n",
      "Episode: 8 | Return: 203.59837448621457\n",
      "Episode: 9 | Return: 209.4784829681353\n",
      "Episode: 10 | Return: 256.11697182020293\n",
      "Episode: 11 | Return: 237.356934021136\n",
      "Episode: 12 | Return: 217.1448463604666\n",
      "Episode: 13 | Return: 226.68471576476642\n",
      "Episode: 14 | Return: 337.9210100875975\n",
      "Episode: 15 | Return: 252.71554139918268\n",
      "Episode: 16 | Return: 237.7844284621892\n",
      "Episode: 17 | Return: 207.32764050123143\n",
      "Episode: 18 | Return: 227.55678428685997\n",
      "Episode: 19 | Return: 244.13151103307416\n",
      "Episode: 20 | Return: 305.16237377144046\n",
      "Episode: 21 | Return: 335.800646150063\n",
      "Episode: 22 | Return: 255.02569906903437\n",
      "Episode: 23 | Return: 329.80525941677627\n",
      "Episode: 24 | Return: 345.9881651271269\n",
      "Episode: 25 | Return: 321.6407311269335\n",
      "Episode: 26 | Return: 312.0880096475468\n",
      "Episode: 27 | Return: 150.21964033585957\n",
      "Episode: 28 | Return: 200.525832921547\n",
      "Episode: 29 | Return: 231.35689844321354\n",
      "Episode: 30 | Return: 336.7757051762062\n",
      "Episode: 31 | Return: 350.2760234295249\n",
      "Episode: 32 | Return: 226.26336508088383\n",
      "Episode: 33 | Return: 228.32138437076162\n",
      "Episode: 34 | Return: 337.5735945401936\n",
      "Episode: 35 | Return: 215.20252325528688\n",
      "Episode: 36 | Return: 226.0596107771378\n",
      "Episode: 37 | Return: 193.36525105284812\n",
      "Episode: 38 | Return: 351.60614528626866\n",
      "Episode: 39 | Return: 207.4484195427279\n",
      "Episode: 40 | Return: 211.93135384191385\n",
      "Episode: 41 | Return: 131.62044482181622\n",
      "Episode: 42 | Return: 245.8824101641553\n",
      "Episode: 43 | Return: 244.59793997672213\n",
      "Episode: 44 | Return: 181.35651321559368\n",
      "Episode: 45 | Return: 208.41304168018524\n",
      "Episode: 46 | Return: 276.37730309431316\n",
      "Episode: 47 | Return: 308.9304257722402\n",
      "Episode: 48 | Return: 196.21007082126155\n",
      "Episode: 49 | Return: 246.19119336590407\n"
     ]
    }
   ],
   "source": [
    "from agent import Agent, Policy\n",
    "\n",
    "reward_list_actor_critic = []\n",
    "\n",
    "model = \"ACTOR_CRITIC_100k.mdl\"\n",
    "\n",
    "policy = Policy(observation_space_dim, action_space_dim)\n",
    "policy.load_state_dict(torch.load(model), strict=True)\n",
    "\n",
    "agent = Agent(policy, device = \"cpu\")\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    done = False\n",
    "    test_reward = 0\n",
    "    env.seed(seed_value + episode)\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "\n",
    "        action, _ = agent.get_action(state, evaluation=True)\n",
    "\n",
    "        state, reward, done, info = env.step(action.detach().cpu().numpy())\n",
    "\n",
    "        test_reward += reward\n",
    "        # env.render()\n",
    "    reward_list_actor_critic.append(test_reward)\n",
    "    print(f\"Episode: {episode} | Return: {test_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REINFORCE: 370 +- 108\n",
      "REINFORCE with baseline: 340 +- 71\n",
      "Actor Critic: 257 +- 56\n"
     ]
    }
   ],
   "source": [
    "# print the mean and std of the returns\n",
    "print(f\"REINFORCE: {round(np.mean(reward_list_reinforce))} +- {round(np.std(reward_list_reinforce))}\")\n",
    "print(f\"REINFORCE with baseline: {round(np.mean(reward_list_reinforce_with_baseline))} +- {round(np.std(reward_list_reinforce_with_baseline))}\")\n",
    "print(f\"Actor Critic: {round(np.mean(reward_list_actor_critic))} +- {round(np.std(reward_list_actor_critic))}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
