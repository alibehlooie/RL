{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Critic\n",
    "\n",
    "code from claude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from env.custom_hopper import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.001):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        state_value = self.critic(state)\n",
    "        return action_probs, state_value\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        action_probs, _ = self.forward(state)\n",
    "        action_probs = action_probs.detach().numpy()\n",
    "        return np.random.choice(self.action_dim, p=action_probs)\n",
    "    \n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        state = torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        done = torch.FloatTensor([done])\n",
    "        \n",
    "        # Compute action probabilities and state values\n",
    "        action_probs, state_value = self.forward(state)\n",
    "        _, next_state_value = self.forward(next_state)\n",
    "        \n",
    "        # Compute advantage\n",
    "        advantage = reward + (1 - done) * 0.99 * next_state_value - state_value\n",
    "        \n",
    "        # Compute losses\n",
    "        actor_loss = -torch.log(action_probs[action]) * advantage.detach()\n",
    "        critic_loss = advantage.pow(2)\n",
    "        \n",
    "        # Compute total loss\n",
    "        loss = actor_loss + critic_loss\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Box([-1. -1. -1.], [1. 1. 1.], (3,), float32)\n",
      "State space: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf], (11,), float64)\n",
      "Dynamics parameters: [2.53429174 3.92699082 2.71433605 5.0893801 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10000/10000 [1:17:43<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CustomHopper-source-v0')\n",
    "\n",
    "print('Action space:', env.action_space)\n",
    "print('State space:', env.observation_space)\n",
    "print('Dynamics parameters:', env.get_parameters())\n",
    "\n",
    "state_dim = env.observation_space.shape[-1]\n",
    "action_dim = env.action_space.shape[-1]\n",
    "\n",
    "agent = ActorCritic(state_dim, action_dim)\n",
    "\n",
    "seed_value = 1234\n",
    "n_train_episodes = 10000\n",
    "\n",
    "# Training loop (simplified)\n",
    "for episode in tqdm(range(n_train_episodes), desc='Training'):\n",
    "    state = env.reset()  # Assuming you have an environment\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.train(state, action, reward, next_state, done)\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(agent.state_dict(), 'actor_critic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: 105.57162020089108\n",
      "Episode 2: 288.60376453326154\n",
      "Episode 3: 139.96577539735327\n",
      "Episode 4: 198.9992705415749\n",
      "Episode 5: 149.65983392700852\n",
      "Episode 6: 151.61105906456245\n",
      "Episode 7: 127.38692081203864\n",
      "Episode 8: 132.7609439894023\n",
      "Episode 9: 129.59071934361077\n",
      "Episode 10: 141.34431025310812\n",
      "Episode 11: 131.24788704653\n",
      "Episode 12: 118.98110658889483\n",
      "Episode 13: 126.31634474312969\n",
      "Episode 14: 155.4756898171278\n",
      "Episode 15: 128.66839334716724\n",
      "Episode 16: 107.34751197153531\n",
      "Episode 17: 171.13049661153067\n",
      "Episode 18: 162.3165250110732\n",
      "Episode 19: 145.93990685139602\n",
      "Episode 20: 196.28645252529003\n",
      "Episode 21: 147.61658372651561\n",
      "Episode 22: 120.9757093729097\n",
      "Episode 23: 101.24868898359226\n",
      "Episode 24: 154.77373471850325\n",
      "Episode 25: 214.28294850390262\n",
      "Episode 26: 251.08424553646054\n",
      "Episode 27: 136.8696882444297\n",
      "Episode 28: 140.55948115718684\n",
      "Episode 29: 164.73178156185182\n",
      "Episode 30: 91.1272921033802\n",
      "Episode 31: 399.0553155307729\n",
      "Episode 32: 178.838940081719\n",
      "Episode 33: 176.57805721519063\n",
      "Episode 34: 123.79015769578612\n",
      "Episode 35: 173.36651244480046\n",
      "Episode 36: 241.80451267665646\n",
      "Episode 37: 152.1926200916358\n",
      "Episode 38: 164.53663075748437\n",
      "Episode 39: 180.822021901668\n",
      "Episode 40: 205.96838484043474\n",
      "Episode 41: 192.84935893024348\n",
      "Episode 42: 136.5263662325398\n",
      "Episode 43: 136.81801090101283\n",
      "Episode 44: 308.41356132365996\n",
      "Episode 45: 156.6159543243702\n",
      "Episode 46: 158.2688085175337\n",
      "Episode 47: 158.96857233676693\n",
      "Episode 48: 194.86547785389672\n",
      "Episode 49: 174.49519804231306\n",
      "Episode 50: 416.8803779001096\n",
      "mean reward: 171.28259052167624 std: 64.75544860085888\n"
     ]
    }
   ],
   "source": [
    "# load the model and test it\n",
    "agent2 = ActorCritic(state_dim, action_dim)\n",
    "agent2.load_state_dict(torch.load('actor_critic.pth'))\n",
    "\n",
    "reward_list = []\n",
    "\n",
    "for i in range(50):\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent2.get_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    reward_list.append(total_reward)\n",
    "    print(f'Episode {i+1}: {total_reward}')\n",
    "\n",
    "print(f\"mean reward: {np.mean(round(reward_list))} std: {np.std(round(reward_list))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
